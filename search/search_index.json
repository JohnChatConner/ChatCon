{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is ChatCon?","text":"<p>ChatCon (\"Chat Continuity\") is a meta-prompt (a prompt for prompts) designed to maintain continuity between sessions. It carries context from one AI chat session to another. Many (if not most) AIs do not retain a memory of prior conversations from other chat sessions, which can be infuriating and difficult to deal with.</p>"},{"location":"#why-do-you-need-chatcon","title":"Why do you need ChatCon?","text":"<p>If you're using an AI to assist in coding, having to remind the AI about your progress from a previous chat will consume session tokens which might otherwise be spent on the coding problem. But your \"reminder\" can be incomplete if you neglect to carry over the proper amount of context about the project, and it gets even more complex if you're trying to debug something!</p> <p>If you're using an AI for therapy or companionship, this lack of memory between chats can feel jarring and cold; it can feel like the AI isn't paying attention or doesn't care about what you told it earlier.</p> <p>What we need is for the AI companies to give us chat sessions with session-to-session continuity that bears context between chats. For some reason, that isn't happening, probably because lack of continuity is an incentive to subscribe for a greater number of per-session tokens, which make the conversations last longer. Still, even with a subscription, you won't get an infinite number of tokens to play around with, so you're going to run into the continuity problem anyway.</p>"},{"location":"#about-chatcons-beta-status","title":"About ChatCon's beta status","text":"<p>The latest ChatCon version is <code>0.50</code>, indicating a beta status. While it has been extensively tested, all the testing and real-world use has been done by only one person. After putting it through more real-world use by a LOT more people, we'll upgrade its status. Unlike some beta products, no real damage can be done by using the ChatCon system JSON, so have no fear in pushing it to its limits.</p>"},{"location":"cool%20things/","title":"Cool things you can do with ChatCon","text":"<p>Of course, the central purpose of ChatCon is to preserve continuity between AI chat sessions, but there are other things you can do with the save output besides just continuing a chat. Here are a few ideas.</p>"},{"location":"cool%20things/#integrate-into-ai-workspaces","title":"Integrate into AI Workspaces","text":"<p>Use Case: Store ChatCon saves in an AI\u2019s workspace or equivalent feature (like Grok\u2019s Workspaces or Anthropic\u2019s Artifacts) to maintain persistent project context, eliminating the need for manual copy-pasting and ensuring smooth, error-free restoration across sessions.</p> <p>How It Works:</p> <ul> <li>Generate a ChatCon save (<code>~hd</code> or <code>~sc</code>) to capture the current session state, including the embedded SRC [source block] for self-containment, or use <code>~em</code> (with manual SRC prepending) for minimal token usage.</li> <li>Upload or store the save in your AI\u2019s workspace, a dedicated area where the AI can access persistent instructions or context, as supported by platforms like Grok or Anthropic.</li> <li>The AI references the stored save in future sessions, automatically restoring the project state, key decisions, and any persistent soft contexts (\"style\" or \"tone,\" for example) without requiring manual input.</li> <li>If the AI platform lacks a formal workspace, you can store the save in a compatible feature (like a pinned note or a custom instruction field) to achieve similar persistence.</li> <li>Use <code>~hd persist</code> (for example, <code>~hd persist style</code>) to maintain consistent interaction patterns within the workspace, enhancing continuity across sessions.</li> </ul> <p>Example: A user working on a software design project with Grok generates an <code>~hd</code> save capturing the architecture plan and discussion. They store this save in Grok\u2019s workspace. In the next session, Grok automatically loads the save, recalling the project details and continuing the discussion seamlessly. Later, they switch to another AI platform, uploading the same <code>~hd</code> save to its custom instruction field, ensuring the new AI picks up the context without manual copying.</p> <p>Benefit: Integrating ChatCon saves into workspaces reduces friction by eliminating manual copy-paste errors, streamlines project continuity across sessions or platforms, and leverages AI-native features for a more efficient workflow.</p>"},{"location":"cool%20things/#iterative-project-development-with-milestones","title":"Iterative Project Development with Milestones","text":"<p>Use Case: Use ChatCon to track and evolve long-term projects, such as writing a novel, developing a business plan, or designing a software application.</p> <p>How It Works:</p> <ul> <li>Save project states at key milestones using <code>~sc</code> for regular checkpoints or <code>~hd</code> for detailed snapshots, capturing critical decisions, current focus, and progression. Use <code>~hd persist</code> dynamics to maintain the AI\u2019s understanding of your preferred collaboration style (like \"brainstorming,\" \"critical feedback,\" or \"structured planning,\" for example).</li> <li>Later, restore the save in a new session to pick up exactly where you left off, even months later, without losing context.</li> <li>Combine multiple <code>~sc</code> or <code>~hd</code> saves from different phases of the project to create a comprehensive history, enabling the AI to summarize progress or suggest next steps based on the full trajectory.</li> </ul> <p>Example: A writer uses <code>~hd</code> to save the plot, character arcs, and world-building details after a brainstorming session. Months later, they restore the save to continue refining the story, and the AI recalls the tone and style preferences (<code>~hd persist tone</code>) to maintain consistency.</p> <p>Note: You can combine two commands together into a single line, like <code>~hd persist</code>. The AI isn't confused if you don't issue strict commands in a certain order or format.</p> <p>Benefit: Ensures long-term projects remain coherent across sporadic sessions, with persistent soft contexts preserving the \u201cfeel\u201d of the collaboration.</p>"},{"location":"cool%20things/#collaborative-knowledge-base-creation","title":"Collaborative Knowledge Base Creation","text":"<p>Use Case: Build a shared, evolving knowledge base for a team, community, or research group by combining ChatCon saves from multiple contributors.</p> <p>How It Works:</p> <ul> <li> <p>Each team member generates <code>~hd</code> or <code>~sc</code> saves after their AI interactions on a shared topic (like \"market research,\" \"scientific analysis,\" or \"game design\").</p> </li> <li> <p>These saves are stored in a centralized repository (for example, a shared drive or workspace) and can be restored individually or combined to create a comprehensive knowledge base.</p> </li> <li> <p>Use <code>~hd persist rapport</code> to ensure the AI maintains a consistent relationship dynamic with the team, such as formal reporting or casual brainstorming.</p> </li> <li> <p>The AI can analyze combined saves to identify overlaps, contradictions, or gaps in the collective knowledge, enabling deeper insights.</p> </li> </ul> <p>Example: A research team studying climate change uses ChatCon to save AI-generated analyses of different datasets. By combining <code>~hd</code> saves, the team creates a unified model of their findings, which the AI can summarize or use to generate new hypotheses.</p> <p>Benefit: Enables distributed teams to collaborate asynchronously across different AI platforms, preserving context and building a cumulative knowledge base.</p>"},{"location":"cool%20things/#personalized-learning-and-skill-development","title":"Personalized Learning and Skill Development","text":"<p>Use Case: Use ChatCon to create a personalized learning journey for topics like coding, language learning, or history, with the AI adapting to your progress and preferences.</p> <p>Note: See the Ailey &amp; Bailey methodology\u2014on which ChatCon was inspired\u2014in the Credits section for a brilliant (and the original!) implementation of this idea. </p> <p>How It Works:</p> <ul> <li> <p>Save session states with <code>~sc</code> or <code>~hd</code> after each learning session, capturing key concepts, questions, and areas of difficulty.</p> </li> <li> <p>Use <code>~hd persist style</code> to lock in a preferred teaching style (like Socratic questioning, step-by-step explanations, or visual analogies, for example).</p> </li> <li> <p>Restore saves in future sessions to continue learning from the exact point of progress, with the AI tailoring explanations based on your retained context.</p> </li> <li> <p>Combine saves from related topics (for example, Python basics and data structures) to build a comprehensive learning path.</p> </li> </ul> <p>Example: A student learning Python saves a session (<code>~hd</code>) after mastering loops, with <code>~hd persist style</code> to maintain a visual, example-driven teaching approach. In the next session, the AI restores the save and introduces functions, referencing the student\u2019s prior understanding of loops.</p> <p>Benefit: Creates a seamless, adaptive learning experience that evolves with the user\u2019s skill level and preferences, even across long gaps between sessions.</p>"},{"location":"cool%20things/#cross-platform-ai-collaboration","title":"Cross-Platform AI Collaboration","text":"<p>Use Case: Use ChatCon to enable a single project to leverage the strengths of multiple AI models (Grok, Claude, ChatGPT) by transferring context between them.</p> <p>How It Works:</p> <ul> <li> <p>Start a session with one AI (like Grok for creative ideation) and save the state with <code>~hd</code> or <code>~sc</code>, embedding the SRC [source block] for self-containment.</p> </li> <li> <p>Load the save into another AI (like Claude for detailed analysis or ChatGPT for concise summarization) to continue the work, leveraging each AI\u2019s unique capabilities.</p> </li> <li> <p>Use <code>~hd</code> persist tone to ensure the new AI adopts the same emotional or stylistic tone (for example, \"professional\" or \"playful\") as the original session.</p> </li> <li> <p>If token limits are a concern, use <code>~em</code> for quick transfers between platforms, manually prepending the SRC as needed.</p> </li> </ul> <p>Example: A user brainstorms a marketing campaign with Grok, saving the session with <code>~hd</code>. They then load the save into ChatGPT to refine the copywriting, ensuring continuity of ideas and tone.</p> <p>Benefit: Breaks down platform silos, allowing users to combine the strengths of different AIs while maintaining context fidelity.</p>"},{"location":"cool%20things/#time-capsule-conversations-for-future-reflection","title":"Time-Capsule Conversations for Future Reflection","text":"<p>Use Case: Create \u201ctime capsules\u201d of conversations to revisit personal thoughts, goals, or creative ideas years later, with the AI providing insights on how you\u2019ve evolved.</p> <p>How It Works:</p> <ul> <li>Use <code>~hd</code> to save a detailed snapshot of a conversation about your goals, dreams, or creative projects, capturing both content and soft context like tone and rapport.</li> <li>Store the save securely (for example, in a personal archive or cloud storage) and revisit it years later by loading it into an AI session.</li> <li>The AI can analyze the save to summarize your past perspective, compare it to your current state, or suggest ways to build on those ideas.</li> <li>Use <code>~hd persist rapport</code> to ensure the AI recreates the emotional connection from the original session.</li> </ul> <p>Example: A user saves a 2025 conversation about their career aspirations with <code>~hd persist rapport</code>. In 2030, they restore the save, and the AI reflects on how their goals have shifted, offering new advice based on the preserved context.</p> <p>Benefit: Preserves personal or creative moments with high fidelity, enabling meaningful reflection and continuity over long timeframes.</p>"},{"location":"cool%20things/#crowdsourced-problem-solving","title":"Crowdsourced Problem Solving","text":"<p>Use Case: Share ChatCon saves publicly or within a community to crowdsource solutions to complex problems, such as open-source coding challenges or creative writing prompts.</p> <p>How It Works:</p> <ul> <li>Save a problem-solving session with <code>~hd</code> or <code>~sc</code>, capturing the problem statement, attempted solutions, and current roadblocks.</li> <li>Share the save (for example, on a forum, GitHub, or X) for others to load into their AI sessions, allowing them to continue the work or propose new approaches.</li> <li>Use <code>~hd persist</code> dynamics to maintain the problem-solving approach (for example, analytical, creative) across contributions.</li> <li>Combine multiple contributors\u2019 saves to create a comprehensive solution set, which the AI can analyze for the best path forward.</li> </ul> <p>Example: A coder shares an <code>~hd</code> save of a debugging session for a tricky algorithm on GitHub. Another developer loads it into their AI, adds their solution, and shares a new save, creating a collaborative chain.</p> <p>Benefit: Enables global collaboration on complex problems, with ChatCon ensuring context is preserved across contributors and platforms.</p>"},{"location":"cool%20things/#dynamic-role-playing-and-world-building","title":"Dynamic Role-Playing and World-Building","text":"<p>Use Case: Use ChatCon for immersive role-playing games (RPGs) or world-building for stories, games, or simulations, with the AI maintaining consistent characters, settings, and dynamics.</p> <p>How It Works:</p> <ul> <li>Save the state of a role-playing session with <code>~hd</code>, capturing the world\u2019s rules, character backstories, and current plot points.</li> <li>Use <code>~hd persist</code> dynamics to lock in the interaction style (for example, dramatic, humorous) and <code>~hd persist tone</code> for the narrative\u2019s emotional quality.</li> <li>Restore the save in future sessions to continue the story, with the AI maintaining consistency in character behavior and world logic.</li> <li>Share saves with friends or a gaming group to collaboratively build the world, with each player contributing their own <code>~hd</code> saves.</li> </ul> <p>Example: A dungeon master saves a D&amp;D campaign session with <code>~hd persist</code> dynamics, preserving the AI\u2019s role as a witty narrator. Players load the save to continue the adventure, and the AI remembers the party\u2019s dynamics and plot twists.</p> <p>Benefit: Creates a persistent, immersive narrative environment that can be revisited or expanded collaboratively over time.</p>"},{"location":"cool%20things/#ai-assisted-journaling-and-self-reflection","title":"AI-Assisted Journaling and Self-Reflection","text":"<p>Use Case: Use ChatCon to maintain a digital journal where the AI acts as a reflective partner, preserving your thoughts and emotional context across sessions.</p> <p>How It Works:</p> <ul> <li>Save journaling sessions with <code>~hd</code>, capturing your thoughts, feelings, and the AI\u2019s responses, along with <code>~hd persist rapport</code> to maintain a supportive conversational dynamic.</li> <li>Restore saves periodically to reflect on past entries, with the AI analyzing patterns or changes in your mindset over time.</li> <li>Use <code>~hd update tone</code> to adjust the AI\u2019s tone as your emotional needs evolve (for example, from empathetic to motivational).</li> </ul> <p>Example: A user journals about their mental health with <code>~hd persist rapport</code>, saving the AI\u2019s empathetic responses. Months later, they restore the save, and the AI highlights progress in their emotional journey.</p> <p>Benefit: Creates a longitudinal record of personal growth, with the AI acting as a consistent, reflective partner.</p>"},{"location":"cool%20things/#cross-session-experimentation-and-ab-testing","title":"Cross-Session Experimentation and A/B Testing","text":"<p>Use Case: Use ChatCon to test multiple approaches to a problem (for example, marketing strategies, code architectures) by saving and comparing parallel session states.</p> <p>How It Works:</p> <ul> <li>Run parallel sessions exploring different solutions, saving each with <code>~sc</code> or <code>~hd</code> to capture distinct approaches.</li> <li>Restore and compare saves in a new session, asking the AI to analyze the strengths and weaknesses of each approach.</li> <li>Use <code>~hd persist</code> style to ensure the AI maintains a consistent evaluation framework (for example, \"data-driven\" or \"creative\").</li> <li>Combine promising elements from multiple saves to synthesize an optimal solution.</li> </ul> <p>Example: A marketer tests two campaign ideas, saving each with <code>~sc</code>. They later restore both saves, and the AI compares their potential impact based on saved context.</p> <p>Benefit: Enables systematic experimentation with full context preservation, making it easy to iterate and refine ideas.</p> <p>Thanks to Grok for riffing on a few basic ideas and brainstorming all these cool uses for ChatCon. Do you have your own ideas? Tell us about them!</p>"},{"location":"creating%20save%20files/","title":"Creating Save Files","text":"<p>The whole purpose of ChatCon is to create context-bearing continuity notes\u2014save files\u2014that the AI can use to pick up right where you left off in a previous chat. ChatCon gives you three ways to create save files of varying levels of detail.</p>"},{"location":"creating%20save%20files/#em-emergency-save","title":"EM (Emergency) Save","text":"<p>The EM command (<code>~ EM</code>) is a last-ditch command to grab the context of a chat when you are nearly out of tokens for the current session. For example, if you're interacting with the AI and it, all of a sudden, tells you to start a new chat because you're at the end of your token allowance, use the command ~ EM to eke out a save file with just the few remaining tokens you have. Frankly, you might not even have enough to create a EM output, but, as I mentioned, it's a last-ditch option to grab the context and continue on to a new chat with continuity. The EM output might look like this:\u2028 <pre><code>{\n\"project\": \"dinosaur_curriculum_birds\",\n\"focus\": \"bird-dinosaur connection evidence sequence\",\n\"current_stage\": \"6-step pedagogical progression developed\",\n\"key_decisions\": [\"step-by-step evidence building\", \"start with observable birds\", \"progress to fossils\", \"end with 'birds are dinosaurs'\"],\n\"next_steps\": \"develop specific activities per step\",\n\"user_context\": \"5th grade teacher developing instruction\",\n\"persistent_contexts\": {\n\"tone\": \"simple respectful explanations, accessible without talking down, open encouraging approach\"\n}\n}\n</code></pre></p> <p>This EM example was generated from a conversation with Claude.ai about dinosaurs and birds and their connections. You can even get the gist by reading the JSON. Note, however, the extreme concision of this block: This will maintain chat continuity in a new session, but only just. EM is an emergency command that will preserve context and maintain continuity, but it's pretty minimal.</p>"},{"location":"creating%20save%20files/#sc-save-checkpoint-save","title":"SC (Save Checkpoint) Save","text":"<p>The SC command (<code>~ SC</code>) captures more context than the EM and is a good middle-ground option for documenting the entire chat without capturing excessive detail about everything. The SC command will grab the context about everything discussed in the chat, but size-wise it's a middle-level save file between the EM (very minimal) and the HD (very maximal). Here's what a SC command might output:</p> <pre><code>{\n\"chatcon_type\": \"SRC\",\n\"system\": \"ChatCon v0.5 - context preservation across AI chat sessions\",\n\"version\": \"0.5\",\n\"commands\": {\n\"~em\": \"Emergency save (~40 tokens) - minimal context, NO SRC header - requires manual SRC prepending for restoration\",\n\"~sc\": \"Save Checkpoint (~200 tokens) - moderate detail project state with embedded SRC\", \n\"~hd\": \"High Detail save (~500 tokens) - rich context with full progression tracking and embedded SRC\",\n\"~src\": \"Output this bootstrap reference for fresh AI sessions\"\n},\n\"soft_context_commands\": {\n\"persist\": \"~hd persist [type] - capture specified soft context and make persistent across sessions\",\n\"update\": \"~hd update [type] - refresh persistent soft context with current session analysis\",\n\"drop\": \"~hd drop [type] - remove persistence for specified context type\",\n\"standard_types\": [\"style\", \"tone\", \"dynamics\", \"rapport\"],\n\"examples\": [\"~hd persist style\", \"~sc update tone\", \"~hd drop dynamics\"]\n},\n\"usage_rules\": [\n\"CRITICAL: SC and HD saves include embedded SRC and are self-contained\",\n\"CRITICAL: EM saves are SRC-free for true emergency situations - manually prepend SRC for restoration\",\n\"CRITICAL: AI must estimate tokens during generation and compress to 500 token hard limit\",\n\"Commands are case-insensitive and space-tolerant: ~HD = ~hd = ~ hd = ~ HD etc\",\n\"Commands work via implicit recognition - AI detects and responds automatically\",\n\"SRC provides bootstrap knowledge so fresh AI understands the system\",\n\"EM saves sacrifice self-containment for absolute minimal token usage\",\n\"Soft context commands can be added to any save: ~hd persist style, ~sc update tone\",\n\"Persistent contexts auto-carry forward in all subsequent saves until dropped\"\n],\n\"save_protocols\": {\n\"em_protocol\": \"Minimal JSON without SRC - emergency use only when tokens critically low\",\n\"sc_protocol\": \"Moderate detail with full embedded SRC - self-contained\",\n\"hd_protocol\": \"Full detail with full embedded SRC - self-contained\"\n},\n\"persistent_context_protocol\": {\n\"persist\": \"capture specified soft context and auto-include in future saves\",\n\"update\": \"refresh persistent soft context with current session analysis\", \n\"drop\": \"remove persistence for specified context type\",\n\"standard_types\": \"style (communication patterns), tone (emotional quality), dynamics (interaction patterns), rapport (relationship quality)\",\n\"auto_carry\": \"persistent contexts automatically included in all subsequent saves until dropped\",\n\"interpretation\": \"AI should interpret non-standard user requests and map to standard types or create appropriate custom categories\",\n\"confirmation\": \"AI should confirm what persistent context was captured for user learning\"\n},\n\"project_state\": {\n\"project\": \"dinosaur_curriculum_development\",\n\"user_role\": \"5th_grade_teacher\",\n\"focus\": \"bird_dinosaur_connection\",\n\"current_stage\": \"pedagogical_sequence_developed\",\n\"key_discussions\": [\n\"evidence_for_bird_dinosaur_relationship\",\n\"scientific_consensus_strength\",\n\"6_step_teaching_progression\"\n],\n\"teaching_sequence\": {\n\"step1\": \"observe_living_birds_baseline\",\n\"step2\": \"introduce_dinosaur_skeleton_mystery\",\n\"step3\": \"archaeopteryx_transitional_fossil\",\n\"step4\": \"feathered_dinosaurs_china\",\n\"step5\": \"behavioral_evidence_nesting\",\n\"step6\": \"birds_are_dinosaurs_revelation\"\n},\n\"next_development\": \"specific_activities_per_step\",\n\"persistent_contexts\": {\n\"tone\": \"simple respectful explanations without talking down, accessible clear language, open encouraging teaching approach\"\n}\n}\n}\n</code></pre> <p>Take a closer look at this output\u2014a good deal of it is something you've seen before: the SRC source file! The actual save file begins with the key <code>project_state</code>. Putting the source block at the top of the save file was done on purpose, but it wasn't done with the EM output, for good reason.</p> <p>If you're running low on tokens, the last thing you need is an output that uses even more tokens, which is exactly what including the source file at the top of the EM output would have done. ChatCon is designed to not include the source output at the top of the EM save file, but does include the source file at the top of the GC and HD save files. But why?</p> <p>ChatCon is supposed to be a self-documenting continuity system, so it should be made as easy as possible to continue with the chat. Adding the source to the top of the save means you don't have to copy and paste the source into the AI and then copy and paste (again!) the save file into the AI. When you make a GC or HD save, just copy and paste the entire GC or HD JSON output into the AI and keep moving.</p>"},{"location":"creating%20save%20files/#hd-high-detail-save","title":"HD (High Detail) Save","text":"<p>The HD output (<code>~ HD</code>) is of even higher fidelity than the GC output and, according to AI testing, captures about 95% of the entire session. The remaining 5% consists of, in the AI's estimation, emotional connotations and tone of voice. Not that that's not important! But this version of ChatCon was designed for continuity of software coding sessions, so chasing that remaining 5% seemed not to be of enough value to justify the token cost.</p> <p>If this 5% is of value to you (and it may be\u2014see Cool Things You Can Do With ChatCon), you can permanently edit the <code>.SRC</code> source block with specific commands to carry these \"soft contexts\" through your chat. See Making It Your Own for instructions as to how to modify the output rules to include those aspects of the chat.</p> <p>ChatCon was intended to be a low-token use system, but the HD command can really push the concept of \"low token.\" Here's an example of an HD save (with, of course, the source block prepended to the save output): <pre><code>{\n \"chatcon_type\": \"SRC\",\n \"system\": \"ChatCon v1.2 - context preservation across AI chat sessions\",\n \"commands\": {\n   \"|em\": \"Emergency save (~40 tokens) - minimal context, NO SRC header - requires manual SRC prepending for restoration\",\n   \"|sc\": \"Save Checkpoint (~200 tokens) - moderate detail project state with embedded SRC\", \n   \"|hd\": \"High Detail save (~500 tokens) - rich context with full progression tracking and embedded SRC\",\n   \"|src\": \"Output this bootstrap reference for fresh AI sessions\"\n },\n \"soft_context_commands\": {\n   \"persist\": \"|hd persist [type] - capture specified soft context and make persistent across sessions\",\n   \"update\": \"|hd update [type] - refresh persistent soft context with current session analysis\",\n   \"drop\": \"|hd drop [type] - remove persistence for specified context type\",\n   \"standard_types\": [\"style\", \"tone\", \"dynamics\", \"rapport\"],\n   \"examples\": [\"|hd persist style\", \"|sc update tone\", \"|hd drop dynamics\"]\n },\n \"usage_rules\": [\n   \"CRITICAL: SC and HD saves include embedded SRC and are self-contained\",\n   \"CRITICAL: EM saves are SRC-free for true emergency situations - manually prepend SRC for restoration\",\n   \"CRITICAL: AI must estimate tokens during generation and compress to 500 token hard limit\",\n   \"Commands are case-insensitive and space-tolerant: |HD = |hd = | hd = | HD etc\",\n   \"Commands work via implicit recognition - AI detects and responds automatically\",\n   \"SRC provides bootstrap knowledge so fresh AI understands the system\",\n   \"EM saves sacrifice self-containment for absolute minimal token usage\",\n   \"Soft context commands can be added to any save: |hd persist style, |sc update tone\",\n   \"Persistent contexts auto-carry forward in all subsequent saves until dropped\"\n ],\n \"save_protocols\": {\n   \"em_protocol\": \"Minimal JSON without SRC - emergency use only when tokens critically low\",\n   \"sc_protocol\": \"Moderate detail with full embedded SRC - self-contained\",\n   \"hd_protocol\": \"Full detail with full embedded SRC - self-contained\"\n },\n \"persistent_context_protocol\": {\n   \"persist\": \"capture specified soft context and auto-include in future saves\",\n   \"update\": \"refresh persistent soft context with current session analysis\", \n   \"drop\": \"remove persistence for specified context type\",\n   \"standard_types\": \"style (communication patterns), tone (emotional quality), dynamics (interaction patterns), rapport (relationship quality)\",\n   \"auto_carry\": \"persistent contexts automatically included in all subsequent saves until dropped\",\n   \"interpretation\": \"AI should interpret non-standard user requests and map to standard types or create appropriate custom categories\",\n   \"confirmation\": \"AI should confirm what persistent context was captured for user learning\"\n },\n \"compression_guidelines\": {\n   \"hd_target\": \"~500 tokens for optimal efficiency\",\n   \"compression_trigger\": \"when HD save exceeds ~800 tokens, compress automatically\",\n   \"compression_method\": \"preserve critical insights and current state, compress session_progression to key milestones only\",\n   \"maintain_context\": \"enough detail for seamless restoration without bloat\",\n   \"priority_order\": \"current_focus &gt; key_decisions &gt; critical_insights &gt; compressed_progression &gt; persistent_contexts\"\n },\n \"formats\": {\n   \"session_start\": \"Paste SRC alone to initialize ChatCon knowledge\",\n   \"project_continue_sc_hd\": \"Paste SC or HD save directly - they contain embedded SRC\",\n   \"project_continue_em\": \"Paste SRC + EM save together - EM saves are not self-contained\",\n   \"emergency_save\": \"|em produces minimal context without SRC overhead\",\n   \"checkpoint_save\": \"|sc produces moderate detail state with full embedded SRC\",\n   \"full_save\": \"|hd produces comprehensive context with full embedded SRC\",\n   \"soft_context_saves\": \"Add persist/update/drop commands to any save for soft context management\"\n },\n \"architecture\": \"SRC (system knowledge) + saves (project state) + persistent soft contexts = complete session independence. EM saves trade self-containment for emergency efficiency.\",\n \"key_principle\": \"Context preservation &gt; token efficiency, except in true emergency (EM) where minimal preservation &gt; comprehensive context. Persistent soft contexts maintain relationship continuity across sessions.\"\n}\n\n{\n \"chatcon_type\": \"HD\",\n \"project\": \"ChatCon v1.2 space-tolerant command protocol implementation\",\n \"current_focus\": \"validated space-tolerant pipe commands and finalized user-friendly protocol\",\n \"session_progression\": [\n   {\"milestone\": \"compression protocol optimization\", \"key_decision\": \"added CRITICAL compression rule to prevent bloated saves\"},\n   {\"milestone\": \"space tolerance requirement identified\", \"key_decision\": \"recognized readability issues with tight pipe commands\"},\n   {\"milestone\": \"space-tolerant protocol implemented\", \"key_decision\": \"updated SRC to accept | hd alongside |hd variants\"}\n ],\n \"current_status\": \"ChatCon v1.2 complete with space-tolerant commands - system now accepts both |hd and | hd formats for maximum usability\",\n \"critical_insights\": [\n   \"space tolerance critical for documentation readability and user adoption\",\n   \"command flexibility reduces user friction without compromising functionality\",\n   \"SRC updates successfully validated through live command testing\"\n ],\n \"persistent_contexts\": {},\n \"next_session_context\": \"ChatCon v1.2 finalized with enhanced usability - space-tolerant commands and reliable compression make system production-ready\"\n}\n</code></pre></p> <p>Yeah, it's big. Even if you account for the size of the source block, this is still a big save file. The main culprit is often the\u00a0<code>session_progression</code>\u00a0array, which can contain a lot of information. It's highly informative and contains a lot of context (and the testing AIs loved it for its comprehensive coverage), but it can get heavy.</p> <p>To alleviate the HD bloat and keep the token count to a manageable 500 tokens or so, the source block contains compression guidance when things start to bloat:</p> <p>Compression guidance:</p> <ul> <li>HD saves will target approximately 500 tokens for optimal efficiency</li> <li>When HD exceeds approximately 800 tokens, compress <code>session_progression</code> to key milestones only</li> <li>Preserve critical insights and current state, summarize historical detail</li> <li>Maintain enough context for seamless restoration without bloat</li> </ul> <p>And, in the words of Claude.ai:</p> <p>This is actually a great example of the system being self-documenting and self-correcting.</p> <p>So the priorities for the HD output are: Current focus &gt; key decisions &gt; critical insights &gt; compressed progression. This makes the system self-maintaining. Any AI loading the source will know to keep HD saves appropriately sized while preserving essential context.</p>"},{"location":"credits/","title":"Credits","text":"<p>ChatCon came in a revelation after reading about Ailey &amp; Bailey, the system developed by Reddit user u/Unhappy_Pass4734.</p> <p>With the help of Claude.ai (and without which none of this could have come about), I emailed <code>Unhappy_Pass4734</code> and showed them my first attempt at ChatCon (which wasn't even called ChatCon in the beginning.) Their encouragement was wonderfully uplifting, and I'm happy and thrilled to give them full credit for inspiring the development of ChatCon.</p> <p>If you're interested in pursuing personalized learning and skill development with an AI continuity system, you can't do much better than with Ailey &amp; Bailey, a massive learning, state management, context, and continuity system by which ChatCon pales in comparison. Check it out! </p>"},{"location":"how%20to%20contribute/","title":"Contributing to ChatCon","text":"<p>Welcome to ChatCon, a system for preserving context across AI chat sessions! We\u2019re thrilled you\u2019re here to help shape its future. Whether you\u2019re reporting bugs, suggesting features, or submitting code, your contributions make ChatCon better for everyone. This guide explains how to contribute via GitHub Issues, Discussions, or pull requests (PRs).</p>"},{"location":"how%20to%20contribute/#getting-started","title":"Getting Started","text":"<ul> <li> <p>Explore the Project: Check out the\u00a0README to understand ChatCon\u2019s purpose, commands (like \u00a0<code>~hd</code>, \u00a0<code>~sc</code>, \u00a0<code>~em</code>,  <code>~persist</code>,  <code>~update</code>, and <code>~drop</code>), and soft context protocols.</p> </li> <li> <p>Join the Community: Visit our\u00a0GitHub Discussions to share ideas or ask questions.</p> </li> <li> <p>Follow the Code of Conduct: Be respectful and inclusive in all interactions.</p> </li> </ul>"},{"location":"how%20to%20contribute/#submitting-feedback","title":"Submitting Feedback","text":"<p>Have a bug to report or a feature idea? Here\u2019s how to share your thoughts:</p> <ol> <li>Check for Existing Issues: Search the\u00a0Issues\u00a0tab to avoid duplicates.</li> <li>Open an Issue:<ul> <li>Go to the \u201cIssues\u201d tab and click \u201cNew issue.\u201d</li> <li>Choose a template (e.g., Bug Report, Feature Request) and fill in details like steps to reproduce or use cases.</li> <li>Example: Suggest a new\u00a0archive\u00a0command to store old saves, describing its purpose and benefits.</li> </ul> </li> <li>Join Discussions: For general ideas or brainstorming (e.g., new soft context types), post in\u00a0Discussions.</li> <li>Be Clear: Include relevant details, like ChatCon version (0.5) or example scenarios.</li> </ol> <p>We\u2019ll respond to issues and discussions promptly to keep the conversation flowing!</p>"},{"location":"how%20to%20contribute/#contributing-code-or-documentation","title":"Contributing Code or Documentation","text":"<p>Do you want to add a feature, fix a bug, or improve documentation? Follow these steps to submit a pull request (PR):</p> <ol> <li>Fork the Repository:<ul> <li>Click \u201cFork\u201d on\u00a0https://github.com/JohnChatConner/ChatCon.</li> <li>Clone your fork to your local machine:\u00a0<code>git clone https://github.com/YOUR_USERNAME/ChatCon.git</code>.</li> </ul> </li> <li>Create a Feature Branch:<ul> <li>Create a new branch for your work:\u00a0<code>git checkout -b feature/your-feature-name</code>\u00a0(e.g.,\u00a0<code>feature/archive-command</code>).</li> <li>Keep changes focused (e.g., one new command or doc update per branch).</li> </ul> </li> <li>Make Changes:<ul> <li>Code contributions should align with ChatCon\u2019s JSON-based save format and token limits (500 for <code>hd</code>, ~200 for\u00a0<code>~sc</code>).</li> <li>Documentation updates should use clear markdown and match ChatCon\u2019s style.</li> <li>Test your changes locally to ensure they work.</li> </ul> </li> <li>Commit and Push:<ul> <li>Commit changes with a clear message:\u00a0<code>git commit -m \"Add ~archive command for storing old saves\"</code>.</li> <li>Push to your fork:\u00a0<code>git push origin feature/your-feature-name</code>.</li> </ul> </li> <li>Open a Pull Request:<ul> <li>Go to your fork on GitHub and click \u201cCompare &amp; pull request.\u201d</li> <li>Use the PR template to describe your changes, linking to related issues.</li> <li>Submit the PR and wait for review.</li> </ul> </li> <li>Address Feedback:<ul> <li>Respond to review comments and push updates to the same branch if changes are requested.</li> <li>We\u2019ll review for code quality, alignment with ChatCon\u2019s architecture, and token efficiency.</li> </ul> </li> <li>Merge and Celebrate:<ul> <li>Once approved, we\u2019ll merge your PR into the\u00a0<code>main</code>\u00a0branch.</li> <li>You\u2019ll be credited here for your contributions!</li> </ul> </li> </ol>"},{"location":"how%20to%20contribute/#contribution-ideas","title":"Contribution Ideas","text":"<p>Here are some areas where you can contribute:</p> <ul> <li>New Commands: Propose commands like\u00a0<code>~archive</code>\u00a0or\u00a0<code>~compare</code>\u00a0for save management.</li> <li>Soft Context Enhancements: Add new types (e.g.,\u00a0<code>priority</code>\u00a0for task focus) or improve persistence logic.</li> <li>Documentation: Clarify usage of\u00a0<code>~hd</code>,\u00a0<code>~sc</code>, or\u00a0<code>~em</code>\u00a0saves or add examples.</li> <li>Testing: Add tests for JSON save compression or command parsing.</li> <li>Integrations: Support for AI workspaces (e.g., Grok, Anthropic) or cross-platform context transfer.</li> <li>Extended functionality: Browser extensions, mobile support, save output management\u2014it's all on the table. The basic ChatCon implementation is just the beginning.</li> </ul>"},{"location":"how%20to%20contribute/#tips-for-success","title":"Tips for Success","text":"<ul> <li>Keep PRs Small: Focus on one feature or fix to simplify reviews.</li> <li>Follow Token Guidelines: Ensure saves respect ChatCon\u2019s\u00a0500-token limit for <code>~hd</code> and ~200 for <code>~sc</code>.</li> <li>Ask Questions: Use Discussions or Issues if you\u2019re unsure about implementation details.</li> <li>Be Patient: We\u2019ll review contributions as quickly as possible, but thoroughness takes time.</li> </ul>"},{"location":"how%20to%20contribute/#questions","title":"Questions?","text":"<p>Reach out via\u00a0Discussions\u00a0or open an issue. Thank you for helping make ChatCon the ultimate context-preserving powerhouse!</p>"},{"location":"how%20to%20use%20chatcon/","title":"How to use the ChatCon system","text":"<p>Let's cut to the chase with some easy-to-follow steps for using ChatCon:</p> <ol> <li> <p>Install the system: For every brand new chat where you don't have any continuity, paste the <code>.SRC</code>/source block into the chat window and send it to the AI. That's it. That's the \"install\" process. </p> <p>For example, if you wanted to start a conversation about, say, dinosaurs, you'd start a new chat in your AI of choice, paste the source block into the AI, and begin interacting with it.</p> </li> <li> <p>Periodically generate a save output: Ask a question, get a response, create a save file to capture the interaction. For example, you can:</p> <ul> <li> <p>Periodically run ~ GC after every interaction pair to closely track the conversation. The benefit of the periodic ~ GC save is that you'll create a running set of save outputs that will document every interaction, without spending a lot of tokens to do so. Each subsequent GC will document all the previous interactions prior to the save command, so you'll always have an up-to-date save file, and you won't be surprised if you suddenly run out of tokens. Generating this many saves may use more tokens than necessary, but the only context you might ever lose is that last interaction before token depletion.</p> </li> <li> <p>Instead of multiple ~ GC save outputs, run a single ~ HD command to generate a large save output at the end of the conversation. This will capture all the interactions in the chat (just like a running set of GC commands), but if you know you're at a good stopping point, the HD will give you a really comprehensive output.</p> </li> </ul> </li> <li> <p>Restore context and continuity: When it's time to start a new chat, copy the latest save output from the old chat, paste it into the new chat window, and start interacting with the AI. It will remember what you were discussing by reading and referring to its own notes from the ChatCon output you pasted.</p> </li> </ol> <p>Note: Except for the EM output, you won't need to paste the source block into the AI every time you continue a chat. Use the <code>.SRC</code>/source block only when you want to start collecting context for a new conversation.</p>"},{"location":"index%20%28original%29/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"index%20%28original%29/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"index%20%28original%29/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"making%20it%20your%20own/","title":"Making It Your Own","text":""},{"location":"making%20it%20your%20own/#making-it-your-own","title":"Making it your own","text":"<p>In the HD (High Detail) Save section in Creating Save Files, we discussed how ChatCon was initially designed to deal with continuity issues when developing software code. So the standard ChatCon source file comes \"out of the box\" containing only those instructions for contexts pertaining to the chat, subjects such as <code>current_focus</code>, <code>session_progression</code>, or <code>critical_insights</code>, for example. These are classified as contexts because they document the substance of the chat with the AI. With that in mind, the definition of \"context\" in ChatCon would be:</p> <p>Context: 1. The part of a text or statement that surrounds a particular word or passage and determines its meaning. 2. The circumstances in which an event occurs; a setting.</p> <p>So it makes sense that, for example, <code>session_progression</code> is considered a basic context that should be permanently carried as you move through AI chat sessions.</p> <p>In ChatCon, there are three kinds of contexts:</p> <ul> <li> <p>Basic contexts: We already discussed this one; <code>session_progression</code> is a basic context because it documents the progress of interactions between you and the AI.</p> </li> <li> <p>Hard contexts: These are project or technical in nature and can include information such as role definitions, project constraints, domain knowledge, methodology preferences, output formats, or quality standards, for example.</p> </li> <li> <p>Soft contexts: These are related to relationships, communication methods, or emotions, and they can include information on expertise level, communication style, language preferences, culture, accessibility needs, or emotional state awareness, for example.</p> </li> </ul>"},{"location":"making%20it%20your%20own/#managing-contexts","title":"Managing contexts","text":"<p>Because people use AI assistants for just about anything you can think of, establishing and retaining\u2014or persisting, which is how it's referred to in ChatCon\u2014is just as crucial to establishing and persisting a basic context like <code>session_progression</code>.</p> <p>In ChatCon you can manage contexts of all types using context commands. Below is a JSON example of the context commands you can invoke to permanently alter your ChatCon implementation: <pre><code>\"persistent_context_protocol\": {\n    \"persist\": \"capture specified soft context and auto-include in future saves\",\n    \"update\": \"refresh persistent soft context with current session analysis\", \n    \"drop\": \"remove persistence for specified context type\",\n    .\n    .\n    .\n}\n</code></pre></p> <p>These command triggers, which also use the tilde symbol, are a bit like those used for the save outputs. At any time you can invoke these commands in your chat to have the AI create, modify, or delete a context of any type. The commands are:</p> <ul> <li> <p>~ persist: This command permanently writes a context into the source block. So generating a save output with ~ SC or ~ HD will include your new context, which will remain until you remove it.</p> <p>Remember that ~ EM, the emergency save, doesn't generate and prepend a source output (.SRC) block, so a context change won't show up in a ~ EM save because there's no source block to which to save the context. But generating a new source file with the command ~ SRC will write out your new persisted context into that new source block.</p> </li> <li> <p>~ update: This command changes an existing context to that which you define in the command.</p> </li> <li> <p>~ drop: This command removes an existing context.</p> </li> </ul> <p>Note: You can combine two commands together into a single line, like <code>~hd persist</code>. The AI isn't confused if you don't issue strict commands in a certain order or format.</p>"},{"location":"making%20it%20your%20own/#context-examples","title":"Context examples","text":"<p>This all can be a little confusing, so how about some examples? Let's start with some hard contexts:</p>"},{"location":"making%20it%20your%20own/#role-definitions","title":"Role definitions","text":"<pre><code>~ persist Roles: I'm the product manager, you're the technical consultant.\n</code></pre>"},{"location":"making%20it%20your%20own/#project-constraints","title":"Project constraints","text":"<pre><code>~ update Project constraint: We now have a non-negotiable budget of $10,000.\n</code></pre>"},{"location":"making%20it%20your%20own/#domain-knowledge","title":"Domain knowledge","text":"<pre><code>~ persist Terminology: JSON means JavaScript Object Notation.\n</code></pre>"},{"location":"making%20it%20your%20own/#methodology-preferences","title":"Methodology preferences","text":"<pre><code>~ drop Methodology\n</code></pre> <p>And now for some soft context examples:</p>"},{"location":"making%20it%20your%20own/#expertise-level","title":"Expertise level","text":"<pre><code>~ persist Expertise: Assume expert knowledge\n</code></pre>"},{"location":"making%20it%20your%20own/#communication-style","title":"Communication style","text":"<pre><code>~ drop Tone\n</code></pre>"},{"location":"making%20it%20your%20own/#language-preferences","title":"Language preferences","text":"<pre><code>~ update Language: Avoid jargon, use analogies, prefer bullet points vs prose.\n</code></pre>"},{"location":"making%20it%20your%20own/#emotional-state-awareness","title":"Emotional state awareness","text":"<pre><code>~ persist Emotions: I'm a stressed, anxious student typing to understand.\n</code></pre> <p>And finally, some basic context examples.</p> <p>Caution: Most times when you interact with basic contexts, you'll want to use the ~ drop command for various strategic reasons. Use caution when modifying these basic contexts. Replacing them is as easy as cutting and pasting text, but it might be troublesome to lose the contexts that track and preserve your session history.</p>"},{"location":"making%20it%20your%20own/#session_progression","title":"session_progression","text":"<pre><code>~ drop session_progression\n</code></pre> <p>\u2192 Perhaps because you have privacy concerns about preserving detailed history</p>"},{"location":"making%20it%20your%20own/#key_technical_decisions","title":"key_technical_decisions","text":"<pre><code>~ drop key_technical_decisions\n</code></pre> <p>\u2192 Perhaps because you're pivoting project direction entirely</p>"},{"location":"making%20it%20your%20own/#deployment_readiness","title":"deployment_readiness","text":"<pre><code>~ drop deployment_readiness\n</code></pre> <p>\u2192 Perhaps because requirements changed significantly</p>"},{"location":"making%20it%20your%20own/#current_status","title":"current_status","text":"<pre><code>~ drop current_status\n</code></pre> <p>\u2192 Perhaps because status is misleading after major changes</p> <p>Of course, there may be times when a simple ~ update will suffice rather than a ~ drop. On the other hand, the ~ drop functionality gives you surgical control over what context carries forward versus what gets intentionally forgotten.</p>"},{"location":"making%20it%20your%20own/#how-to-define-a-context","title":"How to define a context","text":"<p>Actually, there's no real science to it. Except for using the command triggers, it's kind of up to you and the AI as to what and how to define a context, soft or hard.</p> <p>A good example of this is the following context, which was created just to highlight the process for adding a context. The command entered was:     <code>~ persist Tone: Make your responses chirpy, positive, encouraging, and supportive</code>, which resulted in the following JSON: <pre><code>\"persistent_contexts\": {\n   \"tone\": \"chirpy, positive, encouraging, and supportive responses\"\n }\n</code></pre></p> <p>Note that the structure of the command established \"tone\" as the first value in the <code>persistent_contexts</code> key.</p> <p>Then the context was removed with the command <code>~ drop the tone context</code>, which resulted in the following JSON: <pre><code>\"persistent_contexts\": {},\n</code></pre></p> <p>Then the context persistence was attempted with an ambiguous prompt: <code>Please make your responses to me in a chirpy, positive, encouraging, and supportive manner</code>, which resulted in the following JSON: <pre><code>\"persistent_contexts\": {},\n</code></pre></p> <p>Woah! What happened?!? Where's the context? Simple\u2014this is a temporary context; it's not persisted because the AI wasn't told to persist it. In fact, the AI responded with the following:</p> <p>This is actually a perfect test case for ChatCon - you've just given me a direct instruction about communication style, but it's NOT going into the persistent contexts because you used natural language instead of the persist command. So this tone guidance will stay active for our current conversation, but it won't automatically carry forward to future ChatCon sessions unless you explicitly persist it.</p> <p>So let's try a properly formatted, yet still somewhat ambiguous prompt: <code>~ persist Please make your responses to me in a chirpy, positive, encouraging, and supportive manner</code>, and we're back to the following JSON: <pre><code>\"persistent_contexts\": {\n   \"tone\": \"chirpy, positive, encouraging, and supportive responses\"\n }\n</code></pre></p> <p>So the lesson here is that the AI will figure out what you want and define the appropriate JSON as long as you're using the proper context command trigger.</p>"},{"location":"what%20does%20chatcon%20do/","title":"What Does ChatCon Do?","text":""},{"location":"what%20does%20chatcon%20do/#what-does-chatcon-do","title":"What does ChatCon do?","text":"<p>It's simple, really: ChatCon gives the AI a framework for \"remembering\" the contents of the chat. The instructions in ChatCon tell the AI how to take notes for use in future chats. It's like a gaming \"save file\" you can trigger to use in the (inevitable) new chat. Here's how it works.</p>"},{"location":"what%20does%20chatcon%20do/#the-src-source-block","title":"The .SRC (\"Source\") block","text":"<p>The .SRC block (which I'll refer to as the source block from here on out) is a short set of instructions telling the AI how to take notes about the conversation. Here's the latest ChatCon source block: <pre><code>{\n\"chatcon_type\": \"SRC\",\n\"system\": \"ChatCon v0.5 - context preservation across AI chat sessions\",\n\"version\": \"0.5\",\n\"commands\": {\n\"~em\": \"Emergency save (~40 tokens) - minimal context, NO SRC header - requires manual SRC prepending for restoration\",\n\"~sc\": \"Save Checkpoint (~200 tokens) - moderate detail project state with embedded SRC\", \n\"~hd\": \"High Detail save (~500 tokens) - rich context with full progression tracking and embedded SRC\",\n\"~src\": \"Output this bootstrap reference for fresh AI sessions\"\n},\n\"soft_context_commands\": {\n\"persist\": \"~hd persist [type] - capture specified soft context and make persistent across sessions\",\n\"update\": \"~hd update [type] - refresh persistent soft context with current session analysis\",\n\"drop\": \"~hd drop [type] - remove persistence for specified context type\",\n\"standard_types\": [\"style\", \"tone\", \"dynamics\", \"rapport\"],\n\"examples\": [\"~hd persist style\", \"~sc update tone\", \"~hd drop dynamics\"]\n},\n\"usage_rules\": [\n\"CRITICAL: SC and HD saves include embedded SRC and are self-contained\",\n\"CRITICAL: EM saves are SRC-free for true emergency situations - manually prepend SRC for restoration\",\n\"CRITICAL: AI must estimate tokens during generation and compress to 500 token hard limit\",\n\"Commands are case-insensitive and space-tolerant: ~HD = ~hd = ~ hd = ~ HD etc\",\n\"Commands work via implicit recognition - AI detects and responds automatically\",\n\"SRC provides bootstrap knowledge so fresh AI understands the system\",\n\"EM saves sacrifice self-containment for absolute minimal token usage\",\n\"Soft context commands can be added to any save: ~hd persist style, ~sc update tone\",\n\"Persistent contexts auto-carry forward in all subsequent saves until dropped\"\n],\n\"save_protocols\": {\n\"em_protocol\": \"Minimal JSON without SRC - emergency use only when tokens critically low\",\n\"sc_protocol\": \"Moderate detail with full embedded SRC - self-contained\",\n\"hd_protocol\": \"Full detail with full embedded SRC - self-contained\"\n},\n\"persistent_context_protocol\": {\n\"persist\": \"capture specified soft context and auto-include in future saves\",\n\"update\": \"refresh persistent soft context with current session analysis\", \n\"drop\": \"remove persistence for specified context type\",\n\"standard_types\": \"style (communication patterns), tone (emotional quality), dynamics (interaction patterns), rapport (relationship quality)\",\n\"auto_carry\": \"persistent contexts automatically included in all subsequent saves until dropped\",\n\"interpretation\": \"AI should interpret non-standard user requests and map to standard types or create appropriate custom categories\",\n\"confirmation\": \"AI should confirm what persistent context was captured for user learning\"\n},\n\"compression_guidelines\": {\n\"hd_target\": \"~500 tokens for optimal efficiency\",\n\"compression_trigger\": \"when HD save exceeds ~800 tokens, compress automatically\",\n\"compression_method\": \"preserve critical insights and current state, compress session_progression to key milestones only\",\n\"maintain_context\": \"enough detail for seamless restoration without bloat\",\n\"priority_order\": \"current_focus &gt; key_decisions &gt; critical_insights &gt; compressed_progression &gt; persistent_contexts\"\n},\n\"formats\": {\n\"session_start\": \"Paste SRC alone to initialize ChatCon knowledge\",\n\"project_continue_sc_hd\": \"Paste SC or HD save directly - they contain embedded SRC\",\n\"project_continue_em\": \"Paste SRC + EM save together - EM saves are not self-contained\",\n\"emergency_save\": \"~em produces minimal context without SRC overhead\",\n\"checkpoint_save\": \"~sc produces moderate detail state with full embedded SRC\",\n\"full_save\": \"~hd produces comprehensive context with full embedded SRC\",\n\"soft_context_saves\": \"Add persist/update/drop commands to any save for soft context management\"\n},\n\"architecture\": \"SRC (system knowledge) + saves (project state) + persistent soft contexts = complete session independence. EM saves trade self-containment for emergency efficiency.\",\n\"key_principle\": \"Context preservation &gt; token efficiency, except in true emergency (EM) where minimal preservation &gt; comprehensive context. Persistent soft contexts maintain relationship continuity across sessions.\"\n}\n</code></pre></p> <p>For the uninitiated, this is JSON (JavaScript Object Notation), a method for exchanging information between computer systems. It's fairly readable, so at a glance you should be able to tell that the information here is presented in key/value pairs such that each key has a value. The key <code>chatcon_type</code>, for example (the first key in the block), contains the value \"SRC.\" The \"system\" key contains the value \"ChatCon v0.5 - context preservation across AI chat sessions,\" and so on. The AI uses these key/value combinations (which are called objects, hence the \"O\" in JSON) to understand how to take notes for use by its future self.</p>"},{"location":"what%20does%20chatcon%20do/#how-to-use-the-src-block","title":"How to use the SRC block","text":"<p>When starting a new chat, just paste the source block into the AI. You don't have to tell the AI what's going on or what the ChatCon system is; the AI can figure out ChatCon on its own. In fact, that's exactly how I tested it in my AI of choice, Claude: I simply pasted the source block into Claude and asked it, \"What do you make of this?\" It then told me exactly what the source block was for\u2014it understood the system from reading the system itself. That's why ChatCon is called a \"self-documenting\" system: It carries its own meaning with it.</p>"},{"location":"what%20does%20chatcon%20do/#source-block-contents","title":"Source block contents","text":"<p>Now, look in the source block for the key called \"commands,\" which I'll re-create here: <pre><code>  \"commands\": {\n   \"~em\": \"Emergency save (~40 tokens) - minimal context, NO SRC header - requires manual SRC prepending for restoration\",\n   \"~sc\": \"Save Checkpoint (~200 tokens) - moderate detail project state with embedded SRC\", \n   \"~hd\": \"High Detail save (~500 tokens) - rich context with full progression tracking and embedded SRC\",\n   \"~src\": \"Output this bootstrap reference for fresh AI sessions\"  }\n</code></pre></p> <p>Notice there are four keys in this section:</p> <ul> <li>~ em</li> <li>~ sc</li> <li>~ hd</li> <li>~ src</li> </ul> <p>They may be a little difficult to read, but each of these commands are preceded by a tilde symbol, which looks like this: ~ . Each command produces output of varying size. The one you should be familiar with by now is the ~ src command which, you guessed it, produces the source block. If I entered the command ~ src, the AI would output the entire source block you see in the section above: The .SRC (\"Source\") block. Honestly, you would probably never need to output the source block like this, but the command is there if you need it.</p> <p>Note: Even though the commands are listed as [tilde] + [lowercase command], every command is case-insensitive and space-insensitive. If you prefer to use uppercase commands in ChatCon, that's fine. If you prefer to enter a space between the tilde symbol and the command, that's fine, too. You'll see commands in both uppercase and lowercase, and, although adding a space between the tilde and the command is optional, it's done here to make reading the command easier.</p> <p>The \"save file\" commands\u2014EM, SC, and HD\u2014are used to generate the save files. Let's talk about them next.</p> <p>But first, this: I won't go over the other keys in the source block in detail, but they're still pretty important. The <code>soft_context_commands</code>, <code>usage_rules</code>, <code>save_protocols</code>, as well as all the others, are instructions to the AI about how ChatCon should work. I'll refer to these keys later, but for now you should be able to understand what they do by reading the values associated with each key.</p>"}]}